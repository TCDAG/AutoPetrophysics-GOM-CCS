{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model with SP logs\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_baseline import *\n",
    "check_tf_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw, clean, masked = load_train_logs(folder='train_logs_with_SPnorm', padded_length=42000, nfiles=10)\n",
    "\n",
    "train_logs_clean_norm, train_logs_clean_scaler = datascaler(masked)\n",
    "print('Train logs: {}'.format(train_logs_clean_norm.shape))\n",
    "\n",
    "X_train = train_logs_clean_norm[..., [c for c in range(train_logs_clean_norm.shape[-1]) if c != 2]]\n",
    "y_train = np.expand_dims(train_logs_clean_norm[...,2],-1)\n",
    "print('X_train: {} | y_train: {}'.format(X_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nn(kernel_size:int=15, drop=0.2, depths=[16,32,64], in_channels:int=10):\n",
    "    K.clear_session()\n",
    "    def enc_layer(inp, units):\n",
    "        _ = layers.Conv1D(units, kernel_size, padding='same')(inp)\n",
    "        _ = layers.BatchNormalization()(_)\n",
    "        _ = layers.ReLU()(_)\n",
    "        _ = layers.Dropout(drop)(_)\n",
    "        _ = layers.MaxPooling1D(2)(_)\n",
    "        return _\n",
    "    def dec_layer(inp, units):\n",
    "        _ = layers.Conv1D(units, kernel_size, padding='same')(inp)\n",
    "        _ = layers.BatchNormalization()(_)\n",
    "        _ = layers.ReLU()(_)\n",
    "        _ = layers.Dropout(drop)(_)\n",
    "        _ = layers.UpSampling1D(2)(_)\n",
    "        return _\n",
    "    def residual_cat(in1, in2):\n",
    "        _ = layers.Concatenate()([in1, in2])\n",
    "        return _\n",
    "    def out_layer(inp, units):\n",
    "        _ = dec_layer(inp, units)\n",
    "        _ = layers.Conv1D(1, kernel_size, padding='same', activation='linear')(_)\n",
    "        return _\n",
    "    inputs  = layers.Input(shape=(None, in_channels))\n",
    "    masked  = layers.Masking(mask_value=-999)(inputs)\n",
    "    enc1    = enc_layer(masked, depths[0])\n",
    "    enc2    = enc_layer(enc1, depths[1])\n",
    "    zlatent = enc_layer(enc2, depths[2])\n",
    "    dec3    = residual_cat(enc2, dec_layer(zlatent, depths[1]))\n",
    "    dec2    = residual_cat(enc1, dec_layer(dec3, depths[0]))\n",
    "    outputs = out_layer(dec2, 4)\n",
    "    return Model(inputs, outputs, name='baseline_correction_bigpad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "### If loading a pre-trained NN model ###\n",
    "#########################################\n",
    "# model = keras.models.load_model('baseline_correction_model_bigpad.keras')\n",
    "# print('# parameters: {:,}'.format(model.count_params()))\n",
    "\n",
    "#########################################\n",
    "######## If training from scratch #######\n",
    "#########################################\n",
    "model = make_nn()\n",
    "print('# parameters: {:,}'.format(model.count_params()))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "fit = model.fit(X_train, y_train, \n",
    "                epochs           = 100,\n",
    "                batch_size       = 32,\n",
    "                validation_split = 0.2,\n",
    "                shuffle          = True,\n",
    "                verbose          = True)\n",
    "model.save('baseline_correction_model_bigpad.keras')\n",
    "plot_loss(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Transfer Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder  = 'test_logs'\n",
    "out_folder = 'predicted_logs'\n",
    "\n",
    "padded_length = 75000\n",
    "sp_filt_size  = 25\n",
    "csh_percents  = [5, 95]\n",
    "\n",
    "#################################################################################\n",
    "### Un-comment me if using pre-trained model instead of training from scratch ###\n",
    "#################################################################################\n",
    "# model = keras.models.load_model('baseline_correction_model_bigpad.keras')\n",
    "\n",
    "files = os.listdir(in_folder)\n",
    "log_list, k, = {}, 0\n",
    "for file in tqdm(files, desc='Transfer learning predictions', unit=' file(s)'):\n",
    "    log_las = lasio.read('{}/{}'.format(in_folder, file))\n",
    "    if 'SP' not in log_las.keys():\n",
    "        continue\n",
    "    log_df = pd.DataFrame({'DEPT': log_las['DEPT'], 'SP': log_las['SP']})\n",
    "\n",
    "    log = np.ones((1, padded_length, 2))*-999\n",
    "    log[:, log_df.index, :] = log_df.values\n",
    "\n",
    "    clean = np.nan_to_num(log, nan=-999)\n",
    "    clean = np.ma.masked_where(clean==-999, clean)\n",
    "\n",
    "    log, clean, masked = calc_features(log, clean)\n",
    "    log_norm, scalers  = datascaler(masked)\n",
    "\n",
    "    sp_pred = model.predict(log_norm, verbose=False)\n",
    "    sp_pred = signal.medfilt(sp_pred.squeeze(), sp_filt_size)\n",
    "\n",
    "    csh_linear, csh_percentile, csh_window = predict_csh(sp_pred, percentiles=csh_percents)\n",
    "    sp_pred = sp_pred*scalers['sd'][1] + scalers['mu'][1]\n",
    "\n",
    "    sp_pred = sp_pred[log_df.index].squeeze()\n",
    "    csh_pred = csh_window[log_df.index].squeeze()\n",
    "\n",
    "    log_las.append_curve('SP_PRED', sp_pred, unit='mV', descr='Predicted SP from baseline correction')\n",
    "    log_las.append_curve('CSH_PRED', csh_pred, unit='v/v', descr='Estimated Csh from predicted SP')\n",
    "    log_las.write('{}/{}'.format(out_folder, file), version=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(8,8), sharey=True)\n",
    "ax1, ax2 = axs\n",
    "\n",
    "ax1.plot(log_las['SP'], log_las['DEPT'], color='tab:purple')\n",
    "ax1.plot(log_las['SP_NORM'], log_las['DEPT'], color='purple')\n",
    "ax1.plot(log_las['SP_PRED'], log_las['DEPT'])\n",
    "\n",
    "ax2.plot(log_las['VSH_SP'], log_las['DEPT'], 'k')\n",
    "ax2.plot(log_las['CSH_PRED'], log_las['DEPT'], 'r:')\n",
    "\n",
    "ax1.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
