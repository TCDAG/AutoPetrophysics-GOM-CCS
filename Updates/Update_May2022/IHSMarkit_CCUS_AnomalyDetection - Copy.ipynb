{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/GeostatsGuy/GeostatsPy/blob/master/TCG_color_logo.png?raw=true\" width=\"220\" height=\"240\" />\n",
    "\n",
    "</p>\n",
    "\n",
    " \n",
    "\n",
    "## Spatial Correlation-based Anomaly Detection Method for Subsurface Modeling\n",
    "\n",
    "#### Wendi Liu, Graduate Student, The University of Texas at Austin\n",
    "\n",
    "##### [LinkedIn](https://www.linkedin.com/in/wendi-liu-8a3023127/) | [GeostatsPy](https://github.com/GeostatsGuy/GeostatsPy)\n",
    "\n",
    "#### Michael Pyrcz, Associate Professor, University of Texas at Austin \n",
    "\n",
    "##### [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1) | [GeostatsPy](https://github.com/GeostatsGuy/GeostatsPy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "Spatial data analytics provides new opportunities for automated detection of anomalous data for data quality control and subsurface segmentation to reduce uncertainty in spatial models. Solely data-driven anomaly detection methods do not fully integrate spatial concepts such as spatial continuity and data sparsity. Also, data-driven anomaly detection methods are challenged in integrating critical geoscience and engineering expertise knowledge.\n",
    "\n",
    "The proposed spatial anomaly detection method is based on the semivariogram spatial continuity model derived from sparsely sampled well data and geological interpretations. The method calculates the lag joint cumulative probability for each matched pair of spatial data, given their lag vector and the semivariogram under the assumption of bivariate Gaussian distribution. For each combination of paired spatial data, the associated head and tail Gaussian standardized values of a pair of spatial data is mapped to the joint probability density function informed from the lag vector and semivariogram. The paired data are classified as anomalous if the associated head and tail Gaussian standardized values locate within a low probability zone. The anomaly decision threshold can be decided based on a loss function quantifying the cost of overestimation or underestimation. \n",
    "\n",
    "The proposed spatial correlation anomaly detection method is able to integrate domain expertise knowledge through trend and correlogram models with sparse, spatial data to identify anomalous samples, region, segmentation boundaries or facies transition zones. This is a useful automation tool to identify samples in big spatial data to focus professional attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt                          \n",
    "import random as rand\n",
    "import math\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy.stats import norm\n",
    "import geostatspy.GSLIB as GSLIB                       # GSLIB utilies, visualization and wrapper\n",
    "import geostatspy.geostats as geostats                 # GSLIB methods convert to Python\n",
    "from matplotlib import rc\n",
    "import os\n",
    "cmap = plt.cm.inferno\n",
    "\n",
    "\"\"\"\n",
    "Functions\n",
    "\"\"\"\n",
    "from matplotlib.colors import Normalize\n",
    "# Utility function to move the midpoint of a colormap to be around\n",
    "# the values of interest.\n",
    "\n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "# sample with provided X and Y and append to DataFrame   \n",
    "def sample(array,xmin,xmax,ymin,ymax,step,name,df,xcol,ycol):\n",
    "    if array.ndim == 2:\n",
    "        ny = (array.shape[0])\n",
    "        nx = (array.shape[1])\n",
    "    else:\n",
    "        print('Array must be 2D')\n",
    "    x = []; y = []; v = []\n",
    "    nsamp = len(df)\n",
    "    for isamp in range(0,nsamp):\n",
    "        x = df.iloc[isamp][xcol]\n",
    "        y = df.iloc[isamp][ycol]\n",
    "        iy = min(ny - int((y - ymin)/step) - 1,ny-1)\n",
    "        ix = min(int((x - xmin)/step), nx - 1)\n",
    "        v.append(array[iy,ix])\n",
    "    df[name] = v\n",
    "    return(df) \n",
    "# extract regular spaced samples from a model   \n",
    "def regular_sample(array,xmin,xmax,ymin,ymax,step,mx,my,name):\n",
    "    x = []; y = []; v = []; iix = 0; iiy = 0;\n",
    "    xx, yy = np.meshgrid(np.arange(xmin, xmax, step),np.arange(ymax, ymin, -1*step))\n",
    "    iiy = 0\n",
    "    for iy in range(0,ny):\n",
    "        if iiy >= my:\n",
    "            iix = 0\n",
    "            for ix in range(0,nx):\n",
    "                if iix >= mx:\n",
    "                    x.append(xx[ix,iy]);y.append(yy[ix,iy]); v.append(array[ix,iy])\n",
    "                    iix = 0; iiy = 0\n",
    "                iix = iix + 1\n",
    "        iiy = iiy + 1\n",
    "    df = pd.DataFrame(np.c_[x,y,v],columns=['X', 'Y', name])\n",
    "    return(df)\n",
    "\n",
    "# extract random samples from a model  \n",
    "def random_sample(array,xmin,xmax,ymin,ymax,step,nsamp,name):\n",
    "    import random as rand\n",
    "    x = []; y = []; v = []; iix = 0; iiy = 0;\n",
    "    xx, yy = np.meshgrid(np.arange(xmin, xmax, step),np.arange(ymax-1, ymin-1, -1*step))\n",
    "    ny = xx.shape[0]\n",
    "    nx = xx.shape[1]\n",
    "    sample_index = rand.sample(range((nx)*(ny)), nsamp)\n",
    "    for isamp in range(0,nsamp):\n",
    "        iy = int(sample_index[isamp]/ny)\n",
    "        ix = sample_index[isamp] - iy*nx\n",
    "        x.append(xx[iy,ix])\n",
    "        y.append(yy[iy,ix])\n",
    "        v.append(array[iy,ix])\n",
    "    df = pd.DataFrame(np.c_[x,y,v],columns=['X', 'Y', name])\n",
    "    return(df) \n",
    "\n",
    "def sqdist3(x1,y1,z1,x2,y2,z2,ind,rotmat):\n",
    "    \"\"\"Squared Anisotropic Distance Calculation Given Matrix Indicator\n",
    "    \n",
    "    This routine calculates the anisotropic distance between two points \n",
    "    given the coordinates of each point and a definition of the\n",
    "    anisotropy.\n",
    "    \n",
    "    INPUT VARIABLES:\n",
    " \n",
    "    x1,y1,z1         Coordinates of first point\n",
    "    x2,y2,z2         Coordinates of second point\n",
    "    ind              The rotation matrix to use\n",
    "    rotmat           The rotation matrices\"\"\"\n",
    "    \n",
    "    dx = x1 - x2\n",
    "    dy = y1 - y2\n",
    "    dz = z1 - z2\n",
    "    sqdist = 0.0\n",
    "    for i in range(3):\n",
    "        cont = rotmat[ind, i, 0] * dx + rotmat[ind, i, 1] * dy + rotmat[ind, i, 2] * dz\n",
    "        sqdist += cont**2\n",
    "    return sqdist\n",
    "\n",
    "def setrot3(ang1,ang2,ang3,anis1,anis2,ind,rotmat):\n",
    "    \"\"\"Sets up an Anisotropic Rotation Matrix\n",
    "    \n",
    "    Sets up the matrix to transform cartesian coordinates to coordinates\n",
    "    accounting for angles and anisotropy\n",
    "    \n",
    "    INPUT PARAMETERS:\n",
    "\n",
    "    ang1             Azimuth angle for principal direction\n",
    "    ang2             Dip angle for principal direction\n",
    "    ang3             Third rotation angle\n",
    "    anis1            First anisotropy ratio\n",
    "    anis2            Second anisotropy ratio\n",
    "    ind              matrix indicator to initialize\n",
    "    rotmat           rotation matrices\n",
    "    \n",
    "    Converts the input angles to three angles which make more mathematical sense:\n",
    "\n",
    "          alpha   angle between the major axis of anisotropy and the\n",
    "                  E-W axis. Note: Counter clockwise is positive.\n",
    "          beta    angle between major axis and the horizontal plane.\n",
    "                  (The dip of the ellipsoid measured positive down)\n",
    "          theta   Angle of rotation of minor axis about the major axis\n",
    "                  of the ellipsoid.\"\"\"\n",
    "    \n",
    "    DEG2RAD=np.pi/180.0; EPSLON=1e-20\n",
    "    if (ang1 >= 0.0)&(ang1<270.0):\n",
    "        alpha = (90.0 - ang1) * DEG2RAD\n",
    "    else:\n",
    "        alpha = (450.0 - ang1) * DEG2RAD\n",
    "    beta = -1.0 * ang2 *DEG2RAD\n",
    "    theta = ang3 * DEG2RAD\n",
    "    \n",
    "    sina = np.sin(alpha)\n",
    "    sinb = np.sin(beta)\n",
    "    sint = np.sin(theta)\n",
    "    cosa = np.cos(alpha)\n",
    "    cosb = np.cos(beta)\n",
    "    cost = np.cos(theta)\n",
    "    ### Construct the rotation matrix in the required memory\n",
    "    \n",
    "    afac1 = 1.0/max(anis1, EPSLON)\n",
    "    afac2 = 1.0/max(anis2, EPSLON)\n",
    "    rotmat[ind,0,0] = cosb * cosa\n",
    "    rotmat[ind,0,1] = cosb * sina\n",
    "    rotmat[ind,0,2] = -sinb\n",
    "    rotmat[ind,1,0] = afac1*(-cost*sina + sint*sinb*cosa)\n",
    "    rotmat[ind,1,1] = afac1*(cost*cosa + sint*sinb*sina)\n",
    "    rotmat[ind,1,2] = afac1*( sint * cosb)\n",
    "    rotmat[ind,2,0] = afac2*(sint*sina + cost*sinb*cosa)\n",
    "    rotmat[ind,2,1] = afac2*(-sint*cosa + cost*sinb*sina)\n",
    "    rotmat[ind,2,2] = afac2*(cost * cosb)\n",
    "    \n",
    "    return rotmat\n",
    "\n",
    "def cova3(x1,y1,z1,x2,y2,z2,nst,c0,it,cc,aa,rotmat,cmax,ivarg=1,irot=0, MAXNST=4):\n",
    "    \"\"\"Covariance Between Two Points\n",
    "    This function calculated the covariance associated with a variogram\n",
    "  model specified by a nugget effect and nested varigoram structures.\n",
    "  The anisotropy definition can be different for each nested structure.\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    INPUT VARIABLES:\n",
    "\n",
    "    x1,y1,z1         coordinates of first point\n",
    "    x2,y2,z2         coordinates of second point\n",
    "    nst             number of nested structures (maximum of 4)\n",
    "    ivarg            variogram number (set to 1 unless doing cokriging\n",
    "                        or indicator kriging)\n",
    "    MAXNST           size of variogram parameter arrays\n",
    "    c0              isotropic nugget constant\n",
    "    it               type of each nested structure:\n",
    "                       1. spherical model of range a;\n",
    "                       2. exponential model of parameter a;\n",
    "                            i.e. practical range is 3a\n",
    "                       3. gaussian model of parameter a;\n",
    "                            i.e. practical range is a*sqrt(3)\n",
    "                       4. power model of power a (a must be greater than 0  and\n",
    "                            less than 2).  if linear model, a=1,c=slope.\n",
    "    cc               multiplicative factor of each nested structure.\n",
    "                       (sill-c0) for spherical, exponential,and gaussian\n",
    "                       slope for linear model.\n",
    "    aa               parameter \"a\" of each nested structure.\n",
    "    irot             index of the rotation matrix for the first nested \n",
    "                     structure (irot starts from 0; the second nested structure will use\n",
    "                     irot+1, the third irot+2, and so on)\n",
    "    rotmat           rotation matrices\"\"\"\n",
    "    \n",
    "    EPSLON = 1e-10\n",
    "    PMX=1e10\n",
    "    ### Calculate the maximum covariance value (used for zero distances and for power model covariance):\n",
    "    istart = 1+ (ivarg-1) * MAXNST\n",
    "    \n",
    "    for ii in range(nst):##nst[ivarg-1] if ivarg>1\n",
    "        ist = istart+ ii-1\n",
    "        if it[ist] == 4:\n",
    "            cmax = cmax+PMX\n",
    "        else:\n",
    "            cmax = cmax+cc[ist]\n",
    "        \n",
    "    # Check for very small distance\n",
    "    hsqd = sqdist3(x1,y1,z1,x2,y2,z2,irot,rotmat)\n",
    "    if hsqd < EPSLON:\n",
    "        cova = cmax\n",
    "        return cmax,cova\n",
    "    \n",
    "    # Non-zero distance, loop over all the structures\n",
    "    cova = 0.0\n",
    "    for js in range(nst):##nst[ivarg-1] if ivarg>1\n",
    "        ist = istart+js-1\n",
    "        # Compute the appropriate structural distance\n",
    "#         if ist!=0:\n",
    "#             ir = min((irot+js),MAXROT)\n",
    "#             hsqd = sqdist(x1,y1,z1,x2,y2,z2,ir,MAXROT,rotmat)\n",
    "        h = np.sqrt(hsqd)\n",
    "        if it[ist] == 1: ##Spherical\n",
    "            hr = h/aa[ist]\n",
    "            if hr<1:\n",
    "                cova+=cc[ist]*(1.0-hr*(1.5-0.5*hr*hr))\n",
    "        elif it[ist] == 2: ##Exponential\n",
    "            cova += cc[ist]*np.exp(-3.0*h/aa[ist])\n",
    "        elif it[ist] == 3: ##Gaussian\n",
    "            cova += cc[ist]*np.exp(-(3.0*h/aa[ist])*(3.0*h/aa[ist]))\n",
    "        elif it[ist] == 4: ##Power\n",
    "            cova += cmax-cc[ist]*h**aa[ist]\n",
    "            \n",
    "    return cmax, cova\n",
    "### calculate correlogram from experimental variogram\n",
    "def correlavario(x1, y1, x2, y2,nst=2,c0=0,azi =112.5, it=[1,1],cc=[0.4,0.6],hmaj=[170,1100],hmin = [170,400],hvert=[0,0]):\n",
    "    ###Initialization\n",
    "    rotmat = np.zeros((5, 3, 3))\n",
    "    EPSLON = 1.0e-20\n",
    "    MAXNST=4\n",
    "    maxcov=1.0\n",
    "    cmax = c0\n",
    "    ang1 = np.ones((MAXNST,))*azi #azimuth\n",
    "    ang2 = np.zeros((MAXNST,)) #dip\n",
    "    ang3 = np.zeros((MAXNST,)) #plenge\n",
    "    anis1 = np.zeros((MAXNST,))\n",
    "    anis2 = np.zeros((MAXNST,))\n",
    "    \n",
    "    for i in range(nst):\n",
    "        anis1[i] = hmin[i]/max(hmaj[i],EPSLON)\n",
    "        anis2[i] = hvert[i]/max(hmaj[i],EPSLON)\n",
    "        rotmat = setrot3(ang1[i],ang2[i],ang3[i],anis1[i],anis2[i],i,rotmat)\n",
    "    cmax,cova = cova3(x1,y1,0,x2,y2,0,nst,c0,it,cc,hmaj,rotmat,cmax)\n",
    "    \n",
    "    return cova\n",
    "\n",
    "def anomaly_map(\n",
    "    array,\n",
    "    xmin,\n",
    "    xmax,\n",
    "    ymin,\n",
    "    ymax,\n",
    "    step,\n",
    "    vmin,\n",
    "    vmax,\n",
    "    df,\n",
    "    xcol,\n",
    "    ycol,\n",
    "    vcol,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    "    vlabel,\n",
    "    cmap,\n",
    "):\n",
    "    \"\"\"Pixel plot and location map\n",
    "    :param array: ndarray\n",
    "    :param xmin: x axis minimum\n",
    "    :param xmax: x axis maximum\n",
    "    :param ymin: y axis minimum\n",
    "    :param ymax: y axis maximum\n",
    "    :param step: step\n",
    "    :param vmin: TODO\n",
    "    :param vmax: TODO\n",
    "    :param df: dataframe\n",
    "    :param xcol: data for x axis\n",
    "    :param ycol: data for y axis\n",
    "    :param vcol: color, sequence, or sequence of color\n",
    "    :param title: title\n",
    "    :param xlabel: label for x axis\n",
    "    :param ylabel: label for y axis\n",
    "    :param vlabel: TODO\n",
    "    :param cmap: colormap\n",
    "    :return: QuadContourSet\n",
    "    \"\"\"\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(xmin, xmax, step), np.arange(ymax, ymin, -1 * step)\n",
    "    )\n",
    "\n",
    "    cs = plt.contourf(\n",
    "        xx,\n",
    "        yy,\n",
    "        array,\n",
    "        cmap=cmap,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        levels=np.linspace(vmin, vmax, 100),\n",
    "        norm=MidpointNormalize(vmax=vmax, midpoint=0.1)\n",
    "    )\n",
    " \n",
    "    plt.scatter(\n",
    "        df[xcol],\n",
    "        df[ycol],\n",
    "        s=None,\n",
    "        c=df[vcol],\n",
    "        marker=None,\n",
    "        cmap=cmap,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        alpha=0.8,\n",
    "        linewidths=0.8,\n",
    "        verts=None,\n",
    "        edgecolors=\"black\",\n",
    "        norm=MidpointNormalize(vmax=vmax, midpoint=0.1)\n",
    "    )\n",
    "    plt.title(title,y=1.08)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(ymin, ymax)\n",
    "    cbar = plt.colorbar(orientation=\"vertical\")\n",
    "    cbar.set_label(vlabel, rotation=270, labelpad=20)\n",
    "    return cs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the univariate porosity model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\17137\\OneDrive - The University of Texas at Austin\\IHS\\CCUS\")   # set the working directory\n",
    "df = pd.read_csv('df_centroids.csv')\n",
    "df = df[df['Age']=='Miocene'].copy(deep=True)\n",
    "df.head(n=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = -93; xmax = -89 # large\n",
    "ymin = 26; ymax = 28\n",
    "# xmin = -91.5; xmax = -91.37 # resevoir 170\n",
    "# ymin = 26.15; ymax = 26.25\n",
    "# xmin = -92.1; xmax = -91.2 # multiple reservoirs\n",
    "# ymin = 25.8; ymax = 26.7\n",
    "nx = 200; ny = 100\n",
    "\n",
    "xsiz = (xmax-xmin)/nx; ysiz = (ymax-ymin)/ny\n",
    "xmn = xmin + xsiz*0.5; ymn = ymin + ysiz*0.5\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(xmn, xmax, xsiz), np.arange(ymax-ysiz*0.5, ymin, -1 * ysiz))\n",
    "\n",
    "print('xsiz = ' + str(xsiz) + ' , ysiz = ' + str(ysiz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Select and Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifeat = 0\n",
    "features = ['Por','Thick','Area']\n",
    "units = ['%','m','m$^2$']\n",
    "naive_means = [np.average(df[features[0]].values),np.average(df[features[1]].values),np.average(df[features[2]].values)]\n",
    "fmin = [13,0.0,0.0]\n",
    "fmax = [35,25.0,0.07]\n",
    "fname = ['Porosity', 'Thickness', 'Area']\n",
    "\n",
    "plt.subplot(121)\n",
    "GSLIB.locmap_st(df,'X','Y',features[ifeat],xmin,xmax,ymin,ymax,fmin[ifeat],fmax[ifeat],fname[ifeat],'X(m)','Y(m)',fname[ifeat] + ' (' + units[ifeat] + ')',cmap)\n",
    "plt.subplot(122)\n",
    "\n",
    "GSLIB.hist_st(df[features[ifeat]].values,fmin[ifeat],fmax[ifeat],log=False,cumul=False,bins=20,weights=None,xlabel=fname[ifeat] + ' (' + units[ifeat] + ')',title=fname[ifeat])\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2, top=1, wspace=0.2, hspace=0.2)\n",
    "plt.gcf().text(0.35, -0.22, '(a)',fontsize=15, fontweight='bold',family='Arial')\n",
    "plt.gcf().text(1.5, -0.22, '(b)',fontsize=15, fontweight='bold',family='Arial')\n",
    "plt.savefig('Figure3.tif',dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the spatial data feature (porosity) to standard Gaussian random function Z by normal score transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['N'+features[ifeat]], tv, tns = geostats.nscore(df, features[ifeat])\n",
    "plt.subplot(121)\n",
    "GSLIB.hist_st(df[features[ifeat]].values,fmin[ifeat],fmax[ifeat],log=False,cumul=False,bins=20,weights=None,xlabel=fname[ifeat] + ' (' + units[ifeat] + ')',title=fname[ifeat])\n",
    "\n",
    "plt.subplot(122)\n",
    "GSLIB.hist_st(df['N'+features[ifeat]].values,-3,3,log=False,cumul=False,bins=20,weights=None,xlabel=fname[ifeat] + ' (' + units[ifeat] + ')',title=fname[ifeat])\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.8, wspace=0.2, hspace=0.2)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate experimental variogram to see if trend exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin = -9999.; tmax = 9999.                             # no trimming \n",
    "lag_dist = 0.1; lag_tol = 0.2; nlag = 10;            # maximum lag is 700m and tolerance > 1/2 lag distance for smoothing\n",
    "bandh = 9999.9; atol = 22.5                             # no bandwidth, directional variograms\n",
    "isill = 1  \n",
    "azi_mat = [0,22.5,45,67.5,90,112.5,135,157.5]           # directions in azimuth to consider\n",
    "# Arrays to store the results\n",
    "lag = np.zeros((len(azi_mat),nlag+2)); gamma = np.zeros((len(azi_mat),nlag+2)); npp = np.zeros((len(azi_mat),nlag+2));\n",
    "\n",
    "for iazi in range(0,len(azi_mat)):                      # Loop over all directions\n",
    "    lag[iazi,:], gamma[iazi,:], npp[iazi,:] = geostats.gamv(df,\"X\",\"Y\",\"N\"+features[ifeat],tmin,tmax,lag_dist,lag_tol,nlag,azi_mat[iazi],atol,bandh,isill)\n",
    "    plt.subplot(4,2,iazi+1)\n",
    "    plt.plot(lag[iazi,:],gamma[iazi,:],'x',color = 'black',label = 'Azimuth ' +str(azi_mat[iazi]))\n",
    "    plt.plot([0,lag_dist*nlag],[1.0,1.0],color = 'black')\n",
    "    plt.xlabel(r'Lag Distance $\\bf(h)$, (m)')\n",
    "    plt.ylabel(r'$\\gamma \\bf(h)$')\n",
    "    plt.title('Directional NSCORE ' + features[ifeat] + ' Variogram')\n",
    "    plt.xlim([0,lag_dist*nlag])\n",
    "    plt.ylim([0,5])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=4.2, wspace=0.2, hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find major and minor direction and build the variogram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nug = 0.0; nst = 2                                             # 2 nest structure variogram model parameters\n",
    "it1 = 1; cc1 = 0.5; azi1 = 45.0; hmaj1 = 0.4; hmin1 = 0.4\n",
    "it2 = 1; cc2 = 0.5; azi2 = 45.0; hmaj2 = 0.4; hmin2 = 0.4\n",
    "\n",
    "vario = GSLIB.make_variogram(nug,nst,it1,cc1,azi1,hmaj1,hmin1,it2,cc2,azi2,hmaj2,hmin2) # make model object\n",
    "azm1 = 45;                                # project the model in the 045 azimuth\n",
    "index1,h1,gam1,cov1,ro1 = geostats.vmodel(nlag,lag_dist,azm1,vario)\n",
    "azm2 = 135                                                      # project the model in the 135 azimuth\n",
    "index2,h2,gam2,cov2,ro2 = geostats.vmodel(nlag,lag_dist,azm2,vario)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(lag[2,:],gamma[2,:],'x',color = 'black',label = 'Experimental Variogram')\n",
    "plt.plot([0,nlag*lag_dist],[1.0,1.0],color = 'red')\n",
    "plt.plot(h1,gam1,color = 'black',label = 'Variogram Model')\n",
    "plt.xlabel(r'Lag Distance $\\bf(h)$, (m)')\n",
    "plt.ylabel(r'$\\gamma \\bf(h)$')\n",
    "plt.title('Major Direction Azimuth ' + str(azm1) + ' ' + features[ifeat] + ' Variogram')\n",
    "plt.xlim([0,nlag*lag_dist])\n",
    "plt.ylim([0,1.8])\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(lag[6,:],gamma[6,:],'x',color = 'black',label = 'Experimental Variogram')\n",
    "plt.plot([0,nlag*lag_dist],[1.0,1.0],color = 'red')\n",
    "plt.plot(h2,gam2,color = 'black',label = 'Variogram Model')\n",
    "plt.xlabel(r'Lag Distance $\\bf(h)$, (m)')\n",
    "plt.ylabel(r'$\\gamma \\bf(h)$')\n",
    "plt.title('Minor Direction Azimuth ' + str(azm2) + ' ' + features[ifeat] + ' Variogram')\n",
    "plt.xlim([0,nlag*lag_dist])\n",
    "plt.ylim([0,1.8])\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2, top=1, wspace=0.2, hspace=0.3)\n",
    "plt.gcf().text(0.35, -0.22, '(a)',fontsize=15, fontweight='bold',family='Arial')\n",
    "plt.gcf().text(1.5, -0.22, '(b)',fontsize=15, fontweight='bold',family='Arial')\n",
    "plt.savefig('Figure4.tif',dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Anomaly Probability Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct function to calculate biavariate gaussian distribution joint probability density function and anomaly probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_gaussian(rho,head,tail):\n",
    "    z = head**2-2*rho*head*tail+tail**2\n",
    "    prob = np.exp(-z/(2*(1-rho**2)))/(2*np.pi*np.sqrt(1-rho))\n",
    "    return prob\n",
    "\n",
    "def p_value(nx,ny,rho,ztail,zhead):\n",
    "    xx = np.linspace(-4, 4, nx)\n",
    "    yy = np.linspace(-4, 4, ny)\n",
    "    head,tail = np.meshgrid(xx,yy)\n",
    "    prob = multi_gaussian(rho,head,tail)\n",
    "    \n",
    "    s = prob.sum()\n",
    "    I = np.sort(prob,axis=None)[::-1]\n",
    "    joint_density = 0\n",
    "    times=0\n",
    "    \n",
    "    for i in range(len(I)):\n",
    "        joint_density += I[i]/s\n",
    "        if (i!=0)&(I[i-1] == I[i]):\n",
    "            if (head[np.where(prob == I[i])][times+1]>=zhead)&(head[np.where(prob == I[i])][times+1]-8/(nx-1)<zhead)&(tail[np.where(prob == I[i])][times+1]>=ztail)&(tail[np.where(prob == I[i])][times+1]-8/(ny-1)<ztail):\n",
    "                break #8/(nx-1) to find the width of each meshgrid\n",
    "            times += 1 \n",
    "        else:\n",
    "            if (head[np.where(prob == I[i])][0]>=zhead)&(head[np.where(prob == I[i])][0]-8/(nx-1)<zhead)&(tail[np.where(prob == I[i])][0]>=ztail)&(tail[np.where(prob == I[i])][0]-8/(ny-1)<ztail):\n",
    "                break\n",
    "            times=0\n",
    "    return 1-joint_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_vector(ax,df_tail,df_head,p,rho,i,j):\n",
    "    df_mid.iloc[k_num*i+j,:]['X'] = (df_tail['X']+df_head['X'])/2\n",
    "    df_mid.iloc[k_num*i+j,:]['Y'] = (df_tail['Y']+df_head['Y'])/2\n",
    "    df_mid.iloc[k_num*i+j,:]['pvalue'] = p\n",
    "    df_mid.iloc[k_num*i+j,:]['cova'] = rho\n",
    "    df_mid.iloc[k_num*i+j,:][features[ifeat]] = (df_tail[features[ifeat]]+df_head[features[ifeat]])/2\n",
    "    if p <= 0.05:\n",
    "        ax.plot([df_head['X'],df_tail['X']],[df_head['Y'],df_tail['Y']],'r-',alpha = 0.8)\n",
    "    else:\n",
    "        ax.plot([df_head['X'],df_tail['X']],[df_head['Y'],df_tail['Y']],'k-',alpha = 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Anomaly map with k-nearest neighbor data pair construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(df):\n",
    "    \"\"\"\n",
    "    A simple euclidean distance function\n",
    "    \"\"\"\n",
    "    return math.sqrt((df['X'] - data['X']) ** 2+(df['Y'] - data['Y']) ** 2)\n",
    "\n",
    "k_num=3\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(111)\n",
    "df_mid=pd.DataFrame(data={\"pvalue\": np.zeros((k_num*len(df),)),\"feature\": np.zeros((k_num*len(df),)),\"cova\": np.zeros((k_num*len(df),)),\"X\": np.zeros((k_num*len(df),)),\"Y\": np.zeros((k_num*len(df),))})\n",
    "for rows, data in df.iterrows():\n",
    "    dist = df.apply(lambda x: euclidean_distance(x), axis=1)\n",
    "    # Create a new dataframe with distances.\n",
    "    distance_frame = pd.DataFrame(data={\"dist\": dist})\n",
    "    distance_frame.sort_values(\"dist\", inplace=True)\n",
    "    # Find the most similar (the lowest distance to the data is itself, the second smallest is the most similar ones)\n",
    "    local_smallest = distance_frame.iloc[1:k_num+1].index\n",
    "    nearest_neighbor = df.loc[local_smallest]\n",
    "    nearest_neighbor['dist'] = distance_frame.iloc[1:k_num+1].dist\n",
    "    nearest_neighbor = nearest_neighbor.reset_index()\n",
    "    for i,local in nearest_neighbor.iterrows():\n",
    "        rho = correlavario(data['X'], data['Y'], local['X'], local['Y'],nst=2,c0=0,azi = vario['azi1'], \n",
    "            it=[vario['it1'],vario['it2']],cc=[vario['cc1'],vario['cc2']],hmaj=[vario['hmaj1'],vario['hmaj2']],\n",
    "            hmin = [vario['hmin1'],vario['hmin2']],hvert=[0,0])\n",
    "        zhead = data['N'+features[ifeat]]\n",
    "        ztail = local['N'+features[ifeat]]\n",
    "        p = p_value(nx,ny,rho,ztail,zhead)\n",
    "        plot_vector(ax,local,data,p,rho,rows,i)\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(df['X'],df['Y'],alpha=0.5)\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_xlabel('X(m)')\n",
    "ax.set_ylabel('Y(m)')\n",
    "plt.savefig('5 nearestneighbor result.tif',dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### take the average of the p value at the same location\n",
    "df_map_kn = df_mid.groupby(['X','Y'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anomaly kriging map from k-nearest neighbor data pair construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ktype=0                                    #ordinary kriging\n",
    "nxdis = 1; nydis = 1                       # number of grid discretizations for block kriging (not tested)\n",
    "ndmin = 0; ndmax = 30                      # minimum and maximum data for an estimate\n",
    "map_mean=np.mean(df_map_kn.pvalue)  #won't be used if ktype = 1 i.e. ordinary kriging\n",
    "###Kriging map for anomaly probability\n",
    "nug = 0.1; nst = 2\n",
    "it1 = 1; cc1 = 0.5;azi1 = 0; hmaj1 = 0.3; hmin1 =0.3\n",
    "it2 = 1; cc2 = 0.4; azi2 = 0; hmaj2 = 0.3; hmin2 = 0.3\n",
    "kn_map_vario = GSLIB.make_variogram(nug,nst,it1,cc1,azi1,hmaj1,hmin1,it2,cc2,azi2,hmaj2,hmin2)\n",
    "radius = max(hmaj1, hmaj2)\n",
    "est_kn,var_kn = geostats.kb2d(df_map_kn, 'X', 'Y', 'pvalue',tmin,tmax,nx,xmn,xsiz,ny,ymn,ysiz,nxdis,nydis,\n",
    "         ndmin,ndmax,radius,ktype,map_mean,kn_map_vario)\n",
    "#kb2d(df_map_kn, 'X', 'Y', 'pvalue', nx, ny, xsiz,ysiz, nug,nst,it1,cc1,azi1,hmaj1,hmin1, it2,cc2,azi2,hmaj2,hmin2,'output_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_map_kn['X'],df_map_kn['Y'],c=df_map_kn['pvalue'],vmin=0,vmax=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ysiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_kn.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(est_kn,vmin=0,vmax=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_map(est_kn,xmin,xmax,ymin,ymax,xsiz,0.0,1,df_map_kn,'X','Y','pvalue','Probability Map from K-nearest Neighbor Sampling','X(m)','Y(m)','Probability',cmap='inferno')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Anomaly map with Delaunay tessellation data pair construction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(arr1,arr2):##Input: simplex vertices coordinates\n",
    "    \"\"\"\n",
    "    A simple euclidean distance function\n",
    "    \"\"\"\n",
    "    return math.sqrt((arr1[0] - arr2[0]) ** 2+(arr1[1] - arr2[1]) ** 2)\n",
    "\n",
    "def plot_centroid(df_tail,df_head,p,rho,i,j):\n",
    "\n",
    "    df_mid.iloc[3*j+i,:]['X'] = (df_tail['X']+df_head['X'])/2\n",
    "    df_mid.iloc[3*j+i,:]['Y'] = (df_tail['Y']+df_head['Y'])/2\n",
    "    df_mid.iloc[3*j+i,:]['pvalue'] = p\n",
    "    df_mid.iloc[3*j+i,:]['cova'] = rho\n",
    "    df_mid.iloc[3*j+i,:][features[ifeat]] = (df_tail[features[ifeat]]+df_head[features[ifeat]])/2\n",
    "    if p <= 0.05:\n",
    "        ax.plot([df_head['X'],df_tail['X']],[df_head['Y'],df_tail['Y']],'r-',alpha = 0.8)\n",
    "    else:\n",
    "        ax.plot([df_head['X'],df_tail['X']],[df_head['Y'],df_tail['Y']],'k-',alpha = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.zeros((3,))\n",
    "points=df.loc[:,['X','Y']].values\n",
    "tri = Delaunay(points)\n",
    "df_mid=pd.DataFrame(data={\"pvalue\": np.zeros((3*len(points[tri.simplices]),)),features[ifeat]: np.zeros((3*len(points[tri.simplices]),)),\"cova\": np.zeros((3*len(points[tri.simplices]),)),\"X\": np.zeros((3*len(points[tri.simplices]),)),\"Y\": np.zeros((3*len(points[tri.simplices]),))})\n",
    "j = 0\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for simplex in points[tri.simplices]:\n",
    "    zhead = pd.DataFrame()\n",
    "    ztail = pd.DataFrame()\n",
    "    for i in range(3):\n",
    "        if i!=2:\n",
    "            dist[i] = distance(simplex[i],simplex[i+1])\n",
    "            zhead = pd.concat([zhead,df[(df['X']==simplex[i][0])&(df['Y']==simplex[i][1])]])\n",
    "            ztail = pd.concat([ztail,df[(df['X']==simplex[i+1][0])&(df['Y']==simplex[i+1][1])]])\n",
    "        else:\n",
    "            dist[i] = distance(simplex[i],simplex[0])\n",
    "            zhead = pd.concat([zhead,df[(df['X']==simplex[i][0])&(df['Y']==simplex[i][1])]])\n",
    "            ztail = pd.concat([ztail,df[(df['X']==simplex[0][0])&(df['Y']==simplex[0][1])]])\n",
    "    for i in range(3):\n",
    "        rho = correlavario(zhead.iloc[i,:]['X'], zhead.iloc[i,:]['Y'], ztail.iloc[i,:]['X'], ztail.iloc[i,:]['Y'],\n",
    "            nst=2,c0=0,azi = vario['azi1'], \n",
    "            it=[vario['it1'],vario['it2']],cc=[vario['cc1'],vario['cc2']],hmaj=[vario['hmaj1'],vario['hmaj2']],\n",
    "            hmin = [vario['hmin1'],vario['hmin2']],hvert=[0,0])\n",
    "#         cloud = correlated_var(rho,sample_data.NPor.values)\n",
    "#         xmax=cloud[0].max();ymax=cloud[1].max();xmin=cloud[0].min();ymin=cloud[1].min()\n",
    "#         p = p_value(nx,ny,xmin,xmax,ymin,ymax,cloud,ztail.iloc[i,:]['NPor'],zhead.iloc[i,:]['NPor'])\n",
    "        p = p_value(nx,ny,rho,ztail.iloc[i,:]['N'+features[ifeat]],zhead.iloc[i,:]['N'+features[ifeat]])\n",
    "        plot_centroid(ztail.iloc[i,:],zhead.iloc[i,:],p,rho,i,j)\n",
    "    j+=1\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "#x.scatter(df['X'],df['Y'],alpha=0.5)\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_xlabel('X(m)')\n",
    "ax.set_ylabel('Y(m)')\n",
    "plt.savefig('tesselation result.tif',dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### take the average of the p value at the same location\n",
    "df_map = df_mid.groupby(['X','Y'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anomaly kriging map from tessellation data pair construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin = -9999.; tmax = 9999.\n",
    "ktype = 1    #ordinary kriging\n",
    "nxdis = 1; nydis = 1                       # number of grid discretizations for block kriging (not tested)\n",
    "ndmin = 0; ndmax = 30                      # minimum and maximum data for an estimate\n",
    "map_mean=np.mean(df_map.pvalue)  #won't be used if ktype = 1 i.e. ordinary kriging\n",
    "###Kriging map for Pvalue\n",
    "nug = 0;nst = 2\n",
    "it1 = 2; c1 = 0.7;azm_maj1 = 90; hmaj1 = 400; hmin1 =350\n",
    "it2 = 2; c2 = 0.3;azm_maj2=90; hmaj2 = 650; hmin2 = 600\n",
    "map_vario = GSLIB.make_variogram(nug,nst,it1,cc1,azi1,hmaj1,hmin1,it2,cc2,azi2,hmaj2,hmin2)\n",
    "radius = max(hmaj1, hmaj2)\n",
    "\n",
    "est,var = geostats.kb2d(df_map, 'X', 'Y', 'pvalue',tmin,tmax,nx,xmn,xsiz,ny,ymn,ysiz,nxdis,nydis,\n",
    "         ndmin,ndmax,radius,ktype,map_mean,map_vario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Truncate estimated negative value to 0\n",
    "est[np.where(est<0)] = np.zeros((est[np.where(est<0)].shape))\n",
    "# est_kn[np.where(est_kn<0)] = np.zeros((est_kn[np.where(est_kn<0)].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(111)\n",
    "anomaly_map(est,xmin,xmax,ymin,ymax,xsiz,0.0,1,df_map,'X','Y','pvalue','Anomaly Detection ' + features[ifeat] + ',','Latitude (degrees)','Longitude (degrees)','Probability',cmap='plasma')\n",
    "#plt.subplot(122)\n",
    "#anomaly_map(est_kn,xmin,xmax,ymin,ymax,xsiz,0.0,1,df_map_kn,'X','Y','pvalue','Probability Map from K-nearest Neighbor Sampling','X(m)','Y(m)','Probability',cmap='plasma')\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2)\n",
    "\n",
    "plt.savefig('Figure6.tif',dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the anomaly maps difference between k-nearest neighbor and Delaunay tessellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locpix_diff(\n",
    "    array,\n",
    "    xmin,\n",
    "    xmax,\n",
    "    ymin,\n",
    "    ymax,\n",
    "    step,\n",
    "    vmin,\n",
    "    vmax,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    "    vlabel,\n",
    "    cmap,\n",
    "):\n",
    "    \"\"\"Pixel plot and location map, reimplementation in Python of a GSLIB MOD\n",
    "    with Matplotlib methods (version for subplots).\n",
    "    :param array: ndarray\n",
    "    :param xmin: x axis minimum\n",
    "    :param xmax: x axis maximum\n",
    "    :param ymin: y axis minimum\n",
    "    :param ymax: y axis maximum\n",
    "    :param step: step\n",
    "    :param vmin: TODO\n",
    "    :param vmax: TODO\n",
    "    :param df: dataframe\n",
    "    :param xcol: data for x axis\n",
    "    :param ycol: data for y axis\n",
    "    :param vcol: color, sequence, or sequence of color\n",
    "    :param title: title\n",
    "    :param xlabel: label for x axis\n",
    "    :param ylabel: label for y axis\n",
    "    :param vlabel: TODO\n",
    "    :param cmap: colormap\n",
    "    :return: QuadContourSet\n",
    "    \"\"\"\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(xmin, xmax, step), np.arange(ymax, ymin, -1 * step)\n",
    "    )\n",
    "\n",
    "    cs = plt.contourf(\n",
    "        xx,\n",
    "        yy,\n",
    "        array,\n",
    "        cmap=cmap,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        levels=np.linspace(vmin, vmax, 100),\n",
    "    )\n",
    "   \n",
    "#     plt.title(title,y=1.08)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(ymin, ymax)\n",
    "    cbar =plt.colorbar(ticks=np.linspace(vmin,vmax,5))\n",
    "    cbar.set_label(vlabel, rotation=270, labelpad=20)\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diff = est-est_kn\n",
    "cmap = plt.cm.RdBu\n",
    "locpix_diff(diff,xmin,xmax,ymin,ymax,xsiz,-0.6,0.6,'Anomaly Probability Difference Map','X(m)','Y(m)','Difference',cmap=cmap)\n",
    "plt.savefig('Figure7.tif',dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the relationship between feature magnitude and anomaly probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "por_bins = np.linspace(4,16,6)  # set the bin boundaries and then the centroids for plotting\n",
    "por_centroids = np.linspace((por_bins[0]+por_bins[1])*0.5,(por_bins[4]+por_bins[5])*0.5,5)\n",
    "print(por_bins,por_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_map['por_bins'] = pd.cut(df_map['porosity'],por_bins,labels=por_centroids)\n",
    "\n",
    "\n",
    "cond_exp = df_map.groupby('por_bins')['pvalue'].mean()\n",
    "cond_P90 = df_map.groupby('por_bins')['pvalue'].quantile(.9)\n",
    "cond_P10 = df_map.groupby('por_bins')['pvalue'].quantile(.1)\n",
    "\n",
    "plt.plot(por_centroids,cond_exp,color='black')\n",
    "plt.plot(por_centroids,cond_P90,'r--',color='black',linewidth = 1.0)\n",
    "plt.plot(por_centroids,cond_P10,'r--',color='black',linewidth = 1.0)\n",
    "# plt.scatter(df_map['porosity'],df_map['pvalue'],alpha=0.4,edgecolor='white')\n",
    "plt.xlabel('Porosity (%)')\n",
    "plt.ylabel('Probability | Porosity (%)')\n",
    "# t = plt.title('Anomaly Probability Conditional to Porosity')\n",
    "\n",
    "plt.ylim((0,1))\n",
    "plt.grid(True)\n",
    "\n",
    "plt.text(10, 0.25, 'P90')\n",
    "plt.text(10, 0.67, 'Expectation')\n",
    "plt.text(10, 0.93, 'P10')\n",
    "plt.savefig('Figure8.tif',dpi=300,bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Account for dependence of spatial continuity on feature magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the indicator variogram for the bivariate Gaussian distribution to calculate anomaly probability instead of variogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcf(x,z):\n",
    "    \"\"\"This function calculates the value of the function\n",
    "    f( zc, xi)= exp ( -zc**2/ (1+sin(x))) for zc and x\"\"\"\n",
    "    f = np.exp(-z**2/(1+np.sin(x)))\n",
    "    return f\n",
    "\n",
    "def funcg(x,z):\n",
    "    g = np.exp(-x**2/2.0)\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "def simpson(alim,blim,z,f):\n",
    "    if alim == blim:\n",
    "        value = 0\n",
    "    elif blim>alim: \n",
    "        x = np.arange(alim,blim,0.001)\n",
    "        y = f(x,z)\n",
    "        value = integrate.simps(y,x)\n",
    "    else:\n",
    "        x = np.arange(alim,blim,-0.001)\n",
    "        y = f(x,z)\n",
    "        value = integrate.simps(y,x)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauinv(p):\n",
    "    \"\"\"\n",
    "    p    = double precision cumulative probability value: dble(psingle)\n",
    "    xp   = G^-1 (p) in single precision\n",
    "    ierr = 1 - then error situation (p out of range), 0 - OK\"\"\"\n",
    "    lim = 1.0e-10\n",
    "    p0 = -0.322232431088;p1=-1.0; p2=-0.342242088547\n",
    "    p3 = -0.0204231210245;p4=-0.0000453642210148\n",
    "    q0 = 0.0993484626060; q1 = 0.588581570495; q2 = 0.531103462366\n",
    "    q3 = 0.103537752850; q4 = 0.0038560700634\n",
    "    ##Check for an error situation:\n",
    "    ierr = 1\n",
    "    if p<lim:\n",
    "        xp = -1.0e10\n",
    "        return p,xp,ierr\n",
    "    if p>(1.0-lim):\n",
    "        xp = 1.0e10\n",
    "        return xp\n",
    "    ierr = 0\n",
    "    \n",
    "    pp = p\n",
    "    if p>0.5:\n",
    "        pp = 1-pp\n",
    "    xp = 0.0\n",
    "    if p ==0.5:\n",
    "        return xp\n",
    "    y = np.sqrt(np.log(1.0/(pp*pp)))\n",
    "    xp = y+((((y*p4+p3)*y+p2)*y+p1)*y+p0)/((((y*q4+q3)*y+q2)*y+q1)*y+q0)\n",
    "    if p==pp:\n",
    "        xp = -xp\n",
    "    return xp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bigaus(x1, y1, x2, y2, z_centroid, nst=2,c0=0,azi =90, it=[1,1],cc=[0.3,0.7],hmaj=[200,800],hmin = [120,500],hvert=[0,0]):\n",
    "    \"\"\"\n",
    "    ncut:number of cutoffs\n",
    "    zc: cutoffs, in ascending order\"\"\"\n",
    "    ###Initialization\n",
    "    rotmat = np.zeros((5, 3, 3))\n",
    "    DEG2RAD = np.pi/180\n",
    "    EPSLON = 1.0e-20\n",
    "    MAXNST=4\n",
    "    maxcov=1.0\n",
    "    cmax = c0\n",
    "    ang1 = np.ones((MAXNST,))*azi #azimuth\n",
    "    ang2 = np.zeros((MAXNST,)) #dip\n",
    "    ang3 = np.zeros((MAXNST,)) #plenge\n",
    "    anis1 = np.zeros((MAXNST,))\n",
    "    anis2 = np.zeros((MAXNST,))\n",
    "    #### Set up the rotation matrices\n",
    "    for i in range(nst):\n",
    "        anis1[i] = hmin[i]/max(hmaj[i],EPSLON)\n",
    "        anis2[i] = hvert[i]/max(hmaj[i],EPSLON)\n",
    "        rotmat = setrot3(ang1[i],ang2[i],ang3[i],anis1[i],anis2[i],i,rotmat)\n",
    "    #### Convert cutoffs cumulative probabilities to standard normal cutoffs:\n",
    "# Here z_centroid is already a standard normal cutoff\n",
    "    cmax,cova = cova3(x1,y1,0,x2,y2,0,nst,c0,it,cc,hmaj,rotmat,cmax)\n",
    "    ro = cova/maxcov\n",
    "\n",
    "    ci0 = simpson(0.0,np.pi/2,z_centroid,funcf)\n",
    "    p = simpson(0.0,z_centroid,z_centroid,funcg)\n",
    "    p = 0.5 + p/np.sqrt(2*np.pi)\n",
    "    b = np.arcsin(ro)\n",
    "    ci = simpson(0.0, b,z_centroid,funcf)\n",
    "    ri = (ci0-ci)/(2*np.pi)\n",
    "    rop = ri/(p*(1-p))\n",
    "    c = 1-rop\n",
    "    return c\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nx = 50;ny = 50\n",
    "dist = np.zeros((3,))\n",
    "df_mid=pd.DataFrame(data={\"pvalue\": np.zeros((3*len(points[tri.simplices]),)),\"porosity\": np.zeros((3*len(points[tri.simplices]),)),\"cova\": np.zeros((3*len(points[tri.simplices]),)),\"X\": np.zeros((3*len(points[tri.simplices]),)),\"Y\": np.zeros((3*len(points[tri.simplices]),))})\n",
    "j = 0\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(111)\n",
    "for simplex in points[tri.simplices]:\n",
    "    zhead = pd.DataFrame()\n",
    "    ztail = pd.DataFrame()\n",
    "    for i in range(3):\n",
    "        if i!=2:\n",
    "            dist[i] = distance(simplex[i],simplex[i+1])\n",
    "            zhead = pd.concat([zhead,sample_data[(sample_data['X']==simplex[i][0])&(sample_data['Y']==simplex[i][1])]])\n",
    "            ztail = pd.concat([ztail,sample_data[(sample_data['X']==simplex[i+1][0])&(sample_data['Y']==simplex[i+1][1])]])\n",
    "        else:\n",
    "            dist[i] = distance(simplex[i],simplex[0])\n",
    "            zhead = pd.concat([zhead,sample_data[(sample_data['X']==simplex[i][0])&(sample_data['Y']==simplex[i][1])]])\n",
    "            ztail = pd.concat([ztail,sample_data[(sample_data['X']==simplex[0][0])&(sample_data['Y']==simplex[0][1])]])\n",
    "    for i in range(3):\n",
    "        z_centroid = (ztail.iloc[i,:]['NPor']+zhead.iloc[i,:]['NPor'])/2\n",
    "        rho = bigaus(zhead.iloc[i,:]['X'], zhead.iloc[i,:]['Y'], ztail.iloc[i,:]['X'], ztail.iloc[i,:]['Y'],z_centroid)\n",
    "#         cloud = correlated_var(rho,sample_data.NPor.values)\n",
    "#         xmax=cloud[0].max();ymax=cloud[1].max();xmin=cloud[0].min();ymin=cloud[1].min()\n",
    "#         p = p_value(nx,ny,xmin,xmax,ymin,ymax,cloud,ztail.iloc[i,:]['NPor'],zhead.iloc[i,:]['NPor'])\n",
    "        p = p_value(nx,ny,rho,ztail.iloc[i,:]['NPor'],zhead.iloc[i,:]['NPor'])\n",
    "        plot_centroid(ztail.iloc[i,:],zhead.iloc[i,:],p,rho,i,j)\n",
    "    j+=1\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(sample_data['X'],sample_data['Y'],alpha=0.5)\n",
    "ax.set_xlim(0, 2000)\n",
    "ax.set_ylim(0, 2000)\n",
    "ax.set_xlabel('X(m)')\n",
    "ax.set_ylabel('Y(m)')\n",
    "plt.savefig('Figure9.tif',dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### take the average of the p value at the same location\n",
    "df_map_bigaus = df_mid.groupby(['X','Y'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "por_bins = np.linspace(4,16,6)  # set the bin boundaries and then the centroids for plotting\n",
    "por_centroids = np.linspace((por_bins[0]+por_bins[1])*0.5,(por_bins[4]+por_bins[5])*0.5,5)\n",
    "print(por_bins,por_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_map_bigaus['por_bins'] = pd.cut(df_map_bigaus['porosity'],por_bins,labels=por_centroids)\n",
    "\n",
    "\n",
    "cond_exp_bigaus = df_map_bigaus.groupby('por_bins')['pvalue'].mean()\n",
    "cond_P90_bigaus = df_map_bigaus.groupby('por_bins')['pvalue'].quantile(.9)\n",
    "cond_P10_bigaus = df_map_bigaus.groupby('por_bins')['pvalue'].quantile(.1)\n",
    "\n",
    "plt.plot(por_centroids,cond_exp_bigaus,color='black')\n",
    "plt.plot(por_centroids,cond_P90_bigaus,'r--',color='black',linewidth = 1.0)\n",
    "plt.plot(por_centroids,cond_P10_bigaus,'r--',color='black',linewidth = 1.0)\n",
    "# plt.scatter(df_map_bigaus['porosity'],df_map_bigaus['pvalue'],alpha=0.4,edgecolor='white')\n",
    "plt.xlabel('Porosity (%)')\n",
    "plt.ylabel('Probability | Porosity (%)')\n",
    "# t = plt.title('Probability Conditional to Porosity')\n",
    "\n",
    "plt.ylim((0,1))\n",
    "plt.grid(True)\n",
    "\n",
    "plt.text(10, 0.25, 'P90')\n",
    "plt.text(10, 0.55, 'Expectation')\n",
    "plt.text(10, 0.7, 'P10')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare anomaly probability difference from variogram and indicator variogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "stats_n = df_map.groupby('por_bins')['pvalue'].agg(['mean', 'count', 'std'])\n",
    "stats_bigaus = df_map_bigaus.groupby('por_bins')['pvalue'].agg(['mean', 'count', 'std'])\n",
    "\n",
    "\n",
    "ci95_hi = []\n",
    "ci95_lo = []\n",
    "\n",
    "for i in range(5):\n",
    "    m, c, s = stats_n.iloc[i]\n",
    "    if c<100:\n",
    "        ci95_hi.append(m + 2*s/math.sqrt(c))\n",
    "        ci95_lo.append(m - 2*s/math.sqrt(c))\n",
    "    else:\n",
    "        ci95_hi.append(m + 1.96*s/math.sqrt(c))\n",
    "        ci95_lo.append(m - 1.96*s/math.sqrt(c))\n",
    "\n",
    "stats_n['ci95_hi'] = ci95_hi\n",
    "stats_n['ci95_lo'] = ci95_lo\n",
    "\n",
    "ci95_hi = []\n",
    "ci95_lo = []\n",
    "\n",
    "for i in range(5):\n",
    "    m, c, s = stats_bigaus.iloc[i]\n",
    "    if c<100:\n",
    "        ci95_hi.append(m + 2.06*s/math.sqrt(c))\n",
    "        ci95_lo.append(m - 2.06*s/math.sqrt(c))\n",
    "    else:\n",
    "        ci95_hi.append(m + 1.96*s/math.sqrt(c))\n",
    "        ci95_lo.append(m - 1.96*s/math.sqrt(c))\n",
    "\n",
    "stats_bigaus['ci95_hi'] = ci95_hi\n",
    "stats_bigaus['ci95_lo'] = ci95_lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(por_centroids,cond_exp_bigaus,color='skyblue',label='Indicator variogram')\n",
    "plt.plot(por_centroids,cond_exp,color='olive',label='Variogram')\n",
    "plt.plot(por_centroids,stats_n['ci95_lo'],'--',color='olive',linewidth = 1.0)\n",
    "plt.plot(por_centroids,stats_n['ci95_hi'],'--',color='olive',linewidth = 1.0)\n",
    "plt.fill_between(por_centroids,stats_n['ci95_lo'],stats_n['ci95_hi'],color='grey',alpha=0.2)\n",
    "plt.plot(por_centroids,stats_bigaus['ci95_lo'],'--',color='skyblue',linewidth = 1.0)\n",
    "plt.plot(por_centroids,stats_bigaus['ci95_hi'],'--',color='skyblue',linewidth = 1.0)\n",
    "plt.fill_between(por_centroids,stats_bigaus['ci95_lo'],stats_bigaus['ci95_hi'],color='grey',alpha=0.2)\n",
    "\n",
    "plt.xlabel('Porosity (%)')\n",
    "plt.ylabel('Probability | Porosity (%)')\n",
    "# t = plt.title('Anomaly Probability Conditional to Porosity')\n",
    "plt.ylim((0,1))\n",
    "plt.legend()\n",
    "plt.savefig('Figure10.tif',dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 100; ny = 100\n",
    "xsiz = 20;ysiz = 20\n",
    "tmin = -9999.; tmax = 9999.\n",
    "xmn = xsiz * 0.5\n",
    "ymn = ysiz * 0.5\n",
    "ktype = 1    #ordinary kriging\n",
    "nxdis = 1; nydis = 1                       # number of grid discretizations for block kriging (not tested)\n",
    "ndmin = 0; ndmax = 30                      # minimum and maximum data for an estimate\n",
    "map_mean=np.mean(df_map_bigaus.pvalue)  #won't be used if ktype = 1 i.e. ordinary kriging\n",
    "###Kriging map for Pvalue\n",
    "nug = 0;nst = 2\n",
    "it1 = 2; cc1 = 0.7;azi1 = 112.5; hmaj1 = 400; hmin1 =350\n",
    "it2 = 1; cc2 = 0.3;azi2=112.5; hmaj2 = 800; hmin2 = 450\n",
    "bigaus_map_vario = GSLIB.make_variogram(nug,nst,it1,cc1,azi1,hmaj1,hmin1,it2,cc2,azi2,hmaj2,hmin2)\n",
    "radius = max(hmaj1, hmaj2)\n",
    "\n",
    "est_bigaus,var_bigaus  = geostats.kb2d(df_map_bigaus, 'X', 'Y', 'pvalue',tmin,tmax,nx,xmn,xsiz,ny,ymn,ysiz,nxdis,nydis,\n",
    "         ndmin,ndmax,radius,ktype,map_mean,bigaus_map_vario)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Truncate estimated negative value to 0\n",
    "est_bigaus[np.where(est_bigaus<0)] = np.zeros((est_bigaus[np.where(est_bigaus<0)].shape))\n",
    "# est_kn[np.where(est_kn<0)] = np.zeros((est_kn[np.where(est_kn<0)].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_bigaus = est-est_bigaus\n",
    "cmap = plt.cm.RdBu\n",
    "locpix_diff(diff_bigaus,xmin,xmax,ymin,ymax,xsiz,-0.6,0.6,'Anomaly Probability Difference Map','X(m)','Y(m)','Difference',cmap=cmap)\n",
    "plt.savefig('Figure11.tif',dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The proposed hybrid spatial anomaly detection method integrates geoscience and engineering expertise via trend modeling and variogram modeling to assist data-driven spatial segmentation. \n",
    "\n",
    "The use of variogram models, converted to correlograms combined with the bivariate Gaussian distribution of the h-scatter plot, provide a practical method to assess a pairwise spatial sample data joint probability density function and to assign a useful probability for the occurrence of a specific pair of sample data. The influence of the relation between spatial continuity and feature magnitude on the lag joint probability density function is included by integrating the indicator correlogram to update the standard correlogram. \n",
    "\n",
    "After comparing results from the proposed spatial anomaly detection method with different data pair construction and different variogram approaches, we find the output anomaly map is more sensitive to proper data pairs construction with better coverage of area of interest (e.g. Delaunay tessellation) than feature magnitude for the demonstration case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
